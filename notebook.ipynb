{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74de0134",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-17T14:53:11.827219Z",
     "iopub.status.busy": "2025-10-17T14:53:11.826232Z",
     "iopub.status.idle": "2025-10-17T14:53:13.713873Z",
     "shell.execute_reply": "2025-10-17T14:53:13.712900Z"
    },
    "papermill": {
     "duration": 1.893269,
     "end_time": "2025-10-17T14:53:13.715524",
     "exception": false,
     "start_time": "2025-10-17T14:53:11.822255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Táº£i dá»¯ liá»‡u thÃ nh cÃ´ng!\n",
      "\n",
      "ğŸ‰ Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho BÆ°á»›c 2!\n",
      "-----------------------------------------\n",
      "5 dÃ²ng dá»¯ liá»‡u Ä‘áº§u tiÃªn:\n",
      "                              major_name  \\\n",
      "0                   Google Cybersecurity   \n",
      "1                  Google Data Analytics   \n",
      "2             Google Project Management:   \n",
      "3                       IBM Data Science   \n",
      "4  Google Digital Marketing & E-commerce   \n",
      "\n",
      "                                   major_description university_name  country  \\\n",
      "0   Network Security, Python Programming, Linux, ...          google  Unknown   \n",
      "1   Data Analysis, R Programming, SQL, Business C...          google  Unknown   \n",
      "2   Project Management, Strategy and Operations, ...          google  Unknown   \n",
      "3   Python Programming, Data Science, Machine Lea...             ibm  Unknown   \n",
      "4   Digital Marketing, Marketing, Marketing Manag...          google  Unknown   \n",
      "\n",
      "   university_score  \n",
      "0         62.399382  \n",
      "1         62.399382  \n",
      "2         62.399382  \n",
      "3         62.399382  \n",
      "4         62.399382  \n",
      "\n",
      "KÃ­ch thÆ°á»›c dá»¯ liá»‡u cuá»‘i cÃ¹ng: 1084 dÃ²ng vÃ  5 cá»™t.\n"
     ]
    }
   ],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Táº£i 2 bá»™ dá»¯ liá»‡u chÃ­nh ---\n",
    "# DÃ¹ng try-except Ä‘á»ƒ báº¯t lá»—i FileNotFoundError má»™t cÃ¡ch tÆ°á»ng minh\n",
    "try:\n",
    "    df_rankings = pd.read_csv('/kaggle/input/world-university-rankings/cwurData.csv')\n",
    "    df_coursera = pd.read_csv('/kaggle/input/coursera-course-data/coursera_course_dataset_v2_no_null.csv')\n",
    "    print(\"âœ… Táº£i dá»¯ liá»‡u thÃ nh cÃ´ng!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y tá»‡p. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n trong pháº§n 'Data'.\")\n",
    "    print(f\"Chi tiáº¿t lá»—i: {e}\")\n",
    "\n",
    "\n",
    "# --- 2. Chuáº©n hÃ³a cá»™t cho bá»™ dá»¯ liá»‡u Coursera ---\n",
    "# Äá»•i tÃªn cÃ¡c cá»™t gá»‘c ('Title', 'Organization', 'Skills') thÃ nh cÃ¡c tÃªn chuáº©n ('major_name', 'university_name', 'major_description')\n",
    "df_coursera = df_coursera.rename(columns={\n",
    "    'Title': 'major_name',\n",
    "    'Organization': 'university_name',\n",
    "    'Skills': 'major_description'\n",
    "})\n",
    "\n",
    "# Táº¡o má»™t DataFrame má»›i, Ä‘á»™c láº­p báº±ng .copy() Ä‘á»ƒ trÃ¡nh cáº£nh bÃ¡o SettingWithCopyWarning\n",
    "final_courses_df = df_coursera[['major_name', 'university_name', 'major_description']].copy()\n",
    "\n",
    "# LÃ m sáº¡ch dá»¯ liá»‡u trÃªn DataFrame má»›i\n",
    "final_courses_df.dropna(subset=['major_description'], inplace=True)\n",
    "final_courses_df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "# --- 3. Há»£p nháº¥t dá»¯ liá»‡u khÃ³a há»c vá»›i dá»¯ liá»‡u xáº¿p háº¡ng ---\n",
    "# Chuáº©n hÃ³a tÃªn trÆ°á»ng vá» chá»¯ thÆ°á»ng Ä‘á»ƒ khá»›p (merge)\n",
    "df_rankings['institution'] = df_rankings['institution'].str.lower()\n",
    "final_courses_df['university_name'] = final_courses_df['university_name'].str.lower()\n",
    "\n",
    "# Chá»‰ láº¥y cÃ¡c cá»™t cáº§n thiáº¿t tá»« báº£ng xáº¿p háº¡ng\n",
    "df_rankings_simple = df_rankings[['institution', 'country', 'score']]\n",
    "\n",
    "# Há»£p nháº¥t hai DataFrame\n",
    "master_df = pd.merge(\n",
    "    final_courses_df,\n",
    "    df_rankings_simple,\n",
    "    left_on='university_name',\n",
    "    right_on='institution',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- 4. Xá»­ lÃ½ cÃ¡c giÃ¡ trá»‹ bá»‹ thiáº¿u sau khi há»£p nháº¥t ---\n",
    "# Thay vÃ¬ dÃ¹ng inplace=True, gÃ¡n láº¡i giÃ¡ trá»‹ má»™t cÃ¡ch trá»±c tiáº¿p Ä‘á»ƒ an toÃ n hÆ¡n\n",
    "master_df['score'] = master_df['score'].fillna(master_df['score'].mean())\n",
    "master_df['country'] = master_df['country'].fillna('Unknown')\n",
    "\n",
    "# Äá»•i tÃªn cá»™t 'score' thÃ nh 'university_score' cho rÃµ rÃ ng\n",
    "master_df = master_df.rename(columns={'score': 'university_score'})\n",
    "\n",
    "\n",
    "# --- 5. Táº¡o DataFrame cuá»‘i cÃ¹ng ---\n",
    "# Chá»n cÃ¡c cá»™t cuá»‘i cÃ¹ng vÃ  reset index Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n\n",
    "final_df = master_df[['major_name', 'major_description', 'university_name', 'country', 'university_score']]\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nğŸ‰ Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng cho BÆ°á»›c 2!\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(\"5 dÃ²ng dá»¯ liá»‡u Ä‘áº§u tiÃªn:\")\n",
    "print(final_df.head())\n",
    "print(f\"\\nKÃ­ch thÆ°á»›c dá»¯ liá»‡u cuá»‘i cÃ¹ng: {final_df.shape[0]} dÃ²ng vÃ  {final_df.shape[1]} cá»™t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f89e1ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T14:53:13.721347Z",
     "iopub.status.busy": "2025-10-17T14:53:13.720892Z",
     "iopub.status.idle": "2025-10-17T14:53:23.950313Z",
     "shell.execute_reply": "2025-10-17T14:53:23.949280Z"
    },
    "papermill": {
     "duration": 10.233952,
     "end_time": "2025-10-17T14:53:23.952021",
     "exception": false,
     "start_time": "2025-10-17T14:53:13.718069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\r\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.8.3)\r\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\r\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\r\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\r\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Building wheels for collected packages: googletrans\r\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=e841943341b96e0c716e7be46bbd96bd9aeaf5cf6803069dcc65b51f1afe7e06\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\r\n",
      "Successfully built googletrans\r\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\r\n",
      "  Attempting uninstall: hyperframe\r\n",
      "    Found existing installation: hyperframe 6.1.0\r\n",
      "    Uninstalling hyperframe-6.1.0:\r\n",
      "      Successfully uninstalled hyperframe-6.1.0\r\n",
      "  Attempting uninstall: hpack\r\n",
      "    Found existing installation: hpack 4.1.0\r\n",
      "    Uninstalling hpack-4.1.0:\r\n",
      "      Successfully uninstalled hpack-4.1.0\r\n",
      "  Attempting uninstall: h11\r\n",
      "    Found existing installation: h11 0.16.0\r\n",
      "    Uninstalling h11-0.16.0:\r\n",
      "      Successfully uninstalled h11-0.16.0\r\n",
      "  Attempting uninstall: chardet\r\n",
      "    Found existing installation: chardet 5.2.0\r\n",
      "    Uninstalling chardet-5.2.0:\r\n",
      "      Successfully uninstalled chardet-5.2.0\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.10\r\n",
      "    Uninstalling idna-3.10:\r\n",
      "      Successfully uninstalled idna-3.10\r\n",
      "  Attempting uninstall: h2\r\n",
      "    Found existing installation: h2 4.3.0\r\n",
      "    Uninstalling h2-4.3.0:\r\n",
      "      Successfully uninstalled h2-4.3.0\r\n",
      "  Attempting uninstall: httpcore\r\n",
      "    Found existing installation: httpcore 1.0.9\r\n",
      "    Uninstalling httpcore-1.0.9:\r\n",
      "      Successfully uninstalled httpcore-1.0.9\r\n",
      "  Attempting uninstall: httpx\r\n",
      "    Found existing installation: httpx 0.28.1\r\n",
      "    Uninstalling httpx-0.28.1:\r\n",
      "      Successfully uninstalled httpx-0.28.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "huggingface-hub 1.0.0rc2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "google-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "mcp 1.15.0 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "gradio-client 1.11.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "tokenizers 0.21.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "gradio 5.38.1 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "google-genai 1.27.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "openai 1.97.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\r\n",
      "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\r\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "langsmith 0.4.8 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\r\n",
      "dataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n googletrans phiÃªn báº£n á»•n Ä‘á»‹nh\n",
    "!pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7be3fc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T14:53:23.962906Z",
     "iopub.status.busy": "2025-10-17T14:53:23.962621Z",
     "iopub.status.idle": "2025-10-17T14:53:25.682767Z",
     "shell.execute_reply": "2025-10-17T14:53:25.681674Z"
    },
    "papermill": {
     "duration": 1.727412,
     "end_time": "2025-10-17T14:53:25.684194",
     "exception": false,
     "start_time": "2025-10-17T14:53:23.956782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ táº¡o xong ma tráº­n vector cho cÃ¡c ngÃ nh há»c.\n",
      "KÃ­ch thÆ°á»›c ma tráº­n: (1084, 306)\n",
      "\n",
      "ÄÃ£ nÃ¢ng cáº¥p bá»™ mÃ¡y gá»£i Ã½ vá»›i tÃ­nh nÄƒng dá»‹ch tá»± Ä‘á»™ng!\n"
     ]
    }
   ],
   "source": [
    "# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t, thÃªm Translator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from googletrans import Translator\n",
    "\n",
    "# ===================================================================\n",
    "# PHáº¦N 1: XÃ‚Y Dá»°NG MÃ” HÃŒNH ML (KhÃ´ng thay Ä‘á»•i)\n",
    "# ===================================================================\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "major_vectors = vectorizer.fit_transform(final_df['major_description'])\n",
    "print(\"ÄÃ£ táº¡o xong ma tráº­n vector cho cÃ¡c ngÃ nh há»c.\")\n",
    "print(\"KÃ­ch thÆ°á»›c ma tráº­n:\", major_vectors.shape)\n",
    "\n",
    "# Khá»Ÿi táº¡o Ä‘á»‘i tÆ°á»£ng Translator Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng\n",
    "translator = Translator()\n",
    "\n",
    "# ===================================================================\n",
    "# PHáº¦N 2: VIáº¾T CÃC HÃ€M Gá»¢I Ã (Cáº­p nháº­t)\n",
    "# ===================================================================\n",
    "\n",
    "# HÃ€M 1: Lá»ŒC THEO QUY Táº®C (KhÃ´ng thay Ä‘á»•i)\n",
    "def filter_by_rules(df, min_uni_score=60):\n",
    "    eligible_majors = df[df['university_score'] >= min_uni_score].copy()\n",
    "    print(f\"\\nÄang lá»c... TÃ¬m tháº¥y {len(eligible_majors)} ngÃ nh há»c tá»« cÃ¡c trÆ°á»ng cÃ³ Ä‘iá»ƒm >= {min_uni_score}\")\n",
    "    return eligible_majors\n",
    "\n",
    "# HÃ€M 2: Xáº¾P Háº NG THEO Sá» THÃCH (KhÃ´ng thay Ä‘á»•i)\n",
    "def rank_by_interest(df, user_interest, vectorizer_model, major_vectors_matrix):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    user_vector = vectorizer_model.transform([user_interest])\n",
    "    filtered_indices = df.index\n",
    "    similarity_scores = cosine_similarity(user_vector, major_vectors_matrix[filtered_indices])\n",
    "    df['similarity'] = similarity_scores[0]\n",
    "    return df.sort_values(by='similarity', ascending=False)\n",
    "\n",
    "\n",
    "# HÃ€M 3: HÃ€M Tá»”NG Há»¢P (âœ… ÄÃƒ NÃ‚NG Cáº¤P Vá»šI TÃNH NÄ‚NG Dá»ŠCH)\n",
    "def get_recommendations(user_score_input, user_interest_input, top_n=5):\n",
    "    \"\"\"\n",
    "    Káº¿t há»£p cáº£ hai bÆ°á»›c lá»c vÃ  xáº¿p háº¡ng Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½.\n",
    "    Tá»± Ä‘á»™ng dá»‹ch sá»Ÿ thÃ­ch tá»« tiáº¿ng Viá»‡t sang tiáº¿ng Anh.\n",
    "    \"\"\"\n",
    "    # BÆ°á»›c A: Dá»‹ch sá»Ÿ thÃ­ch cá»§a ngÆ°á»i dÃ¹ng\n",
    "    print(f\"\\nSá»Ÿ thÃ­ch cá»§a báº¡n: '{user_interest_input}'\")\n",
    "    translated_interest = translator.translate(user_interest_input, src='vi', dest='en').text\n",
    "    print(f\"---> ÄÃ£ dá»‹ch sang tiáº¿ng Anh: '{translated_interest}'\")\n",
    "\n",
    "    # BÆ°á»›c B: Lá»c theo quy táº¯c cá»©ng (Ä‘iá»ƒm sá»‘)\n",
    "    filtered_majors = filter_by_rules(final_df, min_uni_score=user_score_input)\n",
    "\n",
    "    # BÆ°á»›c C: Xáº¿p háº¡ng cÃ¡c káº¿t quáº£ Ä‘Ã£ lá»c báº±ng sá»Ÿ thÃ­ch Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch\n",
    "    ranked_recommendations = rank_by_interest(\n",
    "        filtered_majors,\n",
    "        translated_interest,  # Sá»­ dá»¥ng vÄƒn báº£n Ä‘Ã£ dá»‹ch\n",
    "        vectorizer,\n",
    "        major_vectors\n",
    "    )\n",
    "\n",
    "    # Tráº£ vá» top N káº¿t quáº£ tá»‘t nháº¥t\n",
    "    return ranked_recommendations.head(top_n)\n",
    "\n",
    "print(\"\\nÄÃ£ nÃ¢ng cáº¥p bá»™ mÃ¡y gá»£i Ã½ vá»›i tÃ­nh nÄƒng dá»‹ch tá»± Ä‘á»™ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43dd79a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T14:53:25.694888Z",
     "iopub.status.busy": "2025-10-17T14:53:25.694434Z",
     "iopub.status.idle": "2025-10-17T14:53:26.592775Z",
     "shell.execute_reply": "2025-10-17T14:53:26.591662Z"
    },
    "papermill": {
     "duration": 0.905545,
     "end_time": "2025-10-17T14:53:26.594350",
     "exception": false,
     "start_time": "2025-10-17T14:53:25.688805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sá»Ÿ thÃ­ch cá»§a báº¡n: 'MÃ¬nh thÃ­ch lÃ m viá»‡c vá»›i dá»¯ liá»‡u, tÃ¬m kiáº¿m quy luáº­t, vÃ  dÃ¹ng Python vá»›i SQL Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y.'\n",
      "---> ÄÃ£ dá»‹ch sang tiáº¿ng Anh: 'I like working with data, finding patterns, and using Python and SQL to build machine learning models.'\n",
      "\n",
      "Äang lá»c... TÃ¬m tháº¥y 196 ngÃ nh há»c tá»« cÃ¡c trÆ°á»ng cÃ³ Ä‘iá»ƒm >= 70\n",
      "\n",
      "\n",
      "================ Káº¾T QUáº¢ Gá»¢I Ã (ÄÃƒ ÄA Dáº NG HÃ“A) ================\n",
      "ğŸ“ NgÃ nh: Application of AI, InsurTech, and Real Estate Technology\n",
      "ğŸ« TrÆ°á»ng: University Of Pennsylvania (USA)\n",
      "âœ¨ Má»©c Ä‘á»™ phÃ¹ há»£p sá»Ÿ thÃ­ch: 0.44\n",
      "------------------------------\n",
      "ğŸ“ NgÃ nh: Data Science Capstone\n",
      "ğŸ« TrÆ°á»ng: Johns Hopkins University (USA)\n",
      "âœ¨ Má»©c Ä‘á»™ phÃ¹ há»£p sá»Ÿ thÃ­ch: 0.43\n",
      "------------------------------\n",
      "ğŸ“ NgÃ nh: Fundamentals of Machine Learning for Healthcare\n",
      "ğŸ« TrÆ°á»ng: Stanford University (USA)\n",
      "âœ¨ Má»©c Ä‘á»™ phÃ¹ há»£p sá»Ÿ thÃ­ch: 0.37\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# HÃ€M Má»šI: ÄA Dáº NG HÃ“A Káº¾T QUáº¢\n",
    "# ===================================================================\n",
    "def diversify_recommendations(recommendations_df, top_n=5):\n",
    "    \"\"\"\n",
    "    Tá»« má»™t danh sÃ¡ch gá»£i Ã½, chá»‰ giá»¯ láº¡i gá»£i Ã½ tá»‘t nháº¥t (Ä‘iá»ƒm similarity cao nháº¥t)\n",
    "    cá»§a má»—i trÆ°á»ng Ä‘áº¡i há»c Ä‘á»ƒ káº¿t quáº£ khÃ´ng bá»‹ trÃ¹ng láº·p.\n",
    "    \"\"\"\n",
    "    # Sáº¯p xáº¿p láº¡i má»™t láº§n ná»¯a Ä‘á»ƒ Ä‘áº£m báº£o hÃ ng cÃ³ Ä‘iá»ƒm cao nháº¥t á»Ÿ trÃªn cÃ¹ng\n",
    "    recommendations_df = recommendations_df.sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # XÃ³a cÃ¡c dÃ²ng bá»‹ trÃ¹ng láº·p 'university_name', chá»‰ giá»¯ láº¡i dÃ²ng Äáº¦U TIÃŠN (cÃ³ Ä‘iá»ƒm cao nháº¥t)\n",
    "    diversified_df = recommendations_df.drop_duplicates(subset=['university_name'], keep='first')\n",
    "    \n",
    "    return diversified_df.head(top_n)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PHáº¦N 1: NHáº¬P THÃ”NG TIN Cá»¦A Báº N (KhÃ´ng thay Ä‘á»•i)\n",
    "# ===================================================================\n",
    "my_score = 70\n",
    "my_interest = \"MÃ¬nh thÃ­ch lÃ m viá»‡c vá»›i dá»¯ liá»‡u, tÃ¬m kiáº¿m quy luáº­t, vÃ  dÃ¹ng Python vá»›i SQL Ä‘á»ƒ xÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y.\"\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PHáº¦N 2: Gá»ŒI AI AGENT VÃ€ Xá»¬ LÃ Káº¾T QUáº¢ (âœ… ÄÃƒ NÃ‚NG Cáº¤P)\n",
    "# ===================================================================\n",
    "\n",
    "# BÆ°á»›c A: Láº¥y má»™t danh sÃ¡ch gá»£i Ã½ lá»›n hÆ¡n (vÃ­ dá»¥: top 20) Ä‘á»ƒ cÃ³ nhiá»u lá»±a chá»n\n",
    "initial_recommendations = get_recommendations(my_score, my_interest, top_n=20)\n",
    "\n",
    "# BÆ°á»›c B: DÃ¹ng hÃ m má»›i Ä‘á»ƒ lá»c vÃ  Ä‘a dáº¡ng hÃ³a káº¿t quáº£, chá»‰ láº¥y top 5 cuá»‘i cÃ¹ng\n",
    "final_recommendations = diversify_recommendations(initial_recommendations, top_n=5)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# PHáº¦N 3: IN Káº¾T QUáº¢ (KhÃ´ng thay Ä‘á»•i)\n",
    "# ===================================================================\n",
    "print(\"\\n\\n================ Káº¾T QUáº¢ Gá»¢I Ã (ÄÃƒ ÄA Dáº NG HÃ“A) ================\")\n",
    "if final_recommendations.empty:\n",
    "    print(\"Ráº¥t tiáº¿c, khÃ´ng tÃ¬m tháº¥y ngÃ nh há»c nÃ o phÃ¹ há»£p vá»›i yÃªu cáº§u cá»§a báº¡n.\")\n",
    "else:\n",
    "    for index, row in final_recommendations.iterrows():\n",
    "        print(f\"ğŸ“ NgÃ nh: {row['major_name']}\")\n",
    "        print(f\"ğŸ« TrÆ°á»ng: {row['university_name'].title()} ({row['country']})\")\n",
    "        print(f\"âœ¨ Má»©c Ä‘á»™ phÃ¹ há»£p sá»Ÿ thÃ­ch: {row['similarity']:.2f}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f3ee64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-17T14:53:26.605370Z",
     "iopub.status.busy": "2025-10-17T14:53:26.605068Z",
     "iopub.status.idle": "2025-10-17T14:53:26.636829Z",
     "shell.execute_reply": "2025-10-17T14:53:26.635637Z"
    },
    "papermill": {
     "duration": 0.038761,
     "end_time": "2025-10-17T14:53:26.638323",
     "exception": false,
     "start_time": "2025-10-17T14:53:26.599562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng 2 tá»‡p: 'tfidf_vectorizer.pkl' vÃ  'final_majors_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# LÆ°u láº¡i mÃ´ hÃ¬nh TF-IDF Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# LÆ°u láº¡i DataFrame Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch cuá»‘i cÃ¹ng\n",
    "final_df.to_csv('final_majors_data.csv', index=False)\n",
    "\n",
    "print(\"âœ… ÄÃ£ lÆ°u thÃ nh cÃ´ng 2 tá»‡p: 'tfidf_vectorizer.pkl' vÃ  'final_majors_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 27,
     "sourceId": 792993,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2709508,
     "sourceId": 4671879,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4289048,
     "sourceId": 7585863,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.813902,
   "end_time": "2025-10-17T14:53:27.362532",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-17T14:53:07.548630",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
